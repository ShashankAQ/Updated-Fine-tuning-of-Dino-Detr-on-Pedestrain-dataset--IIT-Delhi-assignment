{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MGydHzzPDaGj"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/IDEA-Research/DINO.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GWQ24vs8EYpa"
      },
      "outputs": [],
      "source": [
        "!pip install -r /content/DINO/requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xgbkskb2Efcu"
      },
      "outputs": [],
      "source": [
        "!pip install yapf==0.40.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BSdDoRpCEkPC",
        "outputId": "aea15166-4f33-410f-ff6a-b16676f69d68"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy<1.24 in /usr/local/lib/python3.10/dist-packages (1.23.5)\n"
          ]
        }
      ],
      "source": [
        "!pip install 'numpy<1.24'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uqbm426TEunl",
        "outputId": "dad9b0ec-c8d4-421a-adb7-364a32d3ab20"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sl2UgsCkE6zx"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append('/content/DINO')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eEf5IvqiE9Zy"
      },
      "outputs": [],
      "source": [
        "# Step 1: Change to the correct directory\n",
        "%cd /content/DINO/models/dino/ops\n",
        "\n",
        "# Step 2: Run the setup.py command\n",
        "!python setup.py build install"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NSi2meq3GMcX"
      },
      "outputs": [],
      "source": [
        "!python test.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LH0drJnrGuDJ",
        "outputId": "3cec9b9c-8e60-49d8-9425-1be257a4cf83"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.23.5\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "print(np.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XLLMXR8VRQTr"
      },
      "source": [
        "*Make sure a empty folder called COCODIR exists before running the below cell*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cXs7ypxyHqd1",
        "outputId": "aea7697e-c450-4023-f8af-2b2abd29e632"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Annotations saved to /content/DINO/COCODIR/annotations/instances_train2017.json\n",
            "Annotations saved to /content/DINO/COCODIR/annotations/instances_val2017.json\n",
            "Train and validation images and annotations have been successfully created.\n"
          ]
        }
      ],
      "source": [
        "import shutil\n",
        "import os\n",
        "import random\n",
        "import json\n",
        "\n",
        "# Base directory for COCO format in Google Colab\n",
        "base_dir = '/content/DINO/COCODIR'\n",
        "image_target_train_dir = os.path.join(base_dir, 'train2017')\n",
        "image_target_val_dir = os.path.join(base_dir, 'val2017')\n",
        "annotation_dir = os.path.join(base_dir, 'annotations')\n",
        "train_annotation_path = os.path.join(annotation_dir, 'instances_train2017.json')\n",
        "val_annotation_path = os.path.join(annotation_dir, 'instances_val2017.json')\n",
        "\n",
        "# Create directories if they don't exist\n",
        "os.makedirs(image_target_train_dir, exist_ok=True)\n",
        "os.makedirs(image_target_val_dir, exist_ok=True)\n",
        "os.makedirs(annotation_dir, exist_ok=True)  # Ensure annotation directory exists\n",
        "\n",
        "# Load your custom dataset annotations\n",
        "annotation_file = '/content/DINO/random_sample_mavi_2_gt.json'  # Adjust this path if necessary\n",
        "with open(annotation_file, 'r') as f:\n",
        "    coco_data = json.load(f)\n",
        "\n",
        "# Shuffle the images randomly\n",
        "image_data = coco_data['images']\n",
        "random.shuffle(image_data)\n",
        "\n",
        "# Split the images into train (160 images) and val (40 images)\n",
        "train_images = image_data[:160]\n",
        "val_images = image_data[160:200]\n",
        "\n",
        "# Function to copy images to target directory\n",
        "def copy_images(images, target_dir):\n",
        "    for img_info in images:\n",
        "        img_filename = img_info['file_name']\n",
        "        src_path = os.path.join('/content/DINO/Pedestrian_dataset_for_internship_assignment', img_filename)  # Adjust this path if necessary\n",
        "        dst_path = os.path.join(target_dir, img_filename)\n",
        "\n",
        "        if os.path.exists(src_path):\n",
        "            shutil.copy(src_path, dst_path)\n",
        "        else:\n",
        "            print(f\"Image not found: {src_path}\")\n",
        "\n",
        "# Copy train images\n",
        "copy_images(train_images, image_target_train_dir)\n",
        "\n",
        "# Copy validation images\n",
        "copy_images(val_images, image_target_val_dir)\n",
        "\n",
        "# Filter annotations based on the split images\n",
        "def filter_annotations(images, annotations):\n",
        "    image_ids = [img['id'] for img in images]\n",
        "    filtered_annotations = [ann for ann in annotations if ann['image_id'] in image_ids]\n",
        "    return filtered_annotations\n",
        "\n",
        "# Create new annotations for train and val\n",
        "train_annotations = filter_annotations(train_images, coco_data['annotations'])\n",
        "val_annotations = filter_annotations(val_images, coco_data['annotations'])\n",
        "\n",
        "# Create new annotation files in COCO format\n",
        "def create_annotation_file(images, annotations, save_path):\n",
        "    new_coco_structure = {\n",
        "        'images': images,\n",
        "        'annotations': annotations,\n",
        "        'categories': coco_data['categories'],\n",
        "        'info': coco_data.get('info', {}),\n",
        "        'licenses': coco_data.get('licenses', [])\n",
        "    }\n",
        "\n",
        "    with open(save_path, 'w') as f:\n",
        "        json.dump(new_coco_structure, f)\n",
        "    print(f\"Annotations saved to {save_path}\")\n",
        "\n",
        "# Save train and val annotations\n",
        "create_annotation_file(train_images, train_annotations, train_annotation_path)\n",
        "create_annotation_file(val_images, val_annotations, val_annotation_path)\n",
        "\n",
        "print(\"Train and validation images and annotations have been successfully created.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8i0oH9tWJtfH",
        "outputId": "5c2e104b-2a35-4428-f25f-878bb03ccd6b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/DINO\n"
          ]
        }
      ],
      "source": [
        "%cd /content/DINO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tnEby14qK0Ib"
      },
      "source": [
        "# **Evaluation for 12 epoch setting**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nlI40SVsKUHJ"
      },
      "outputs": [],
      "source": [
        "\n",
        "coco_path = \"/content/DINO/COCODIR\"\n",
        "checkpoint_path = \"/content/drive/MyDrive/checkpoint0011_4scale.pth\"\n",
        "eval_script_path = \"/content/DINO/scripts/DINO_eval.sh\"\n",
        "\n",
        "!bash {eval_script_path} {coco_path} {checkpoint_path}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DYCDm1xwK9sD"
      },
      "source": [
        "# **Evaluation for 24 epoch setting**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tR0mSLUMK8OA"
      },
      "outputs": [],
      "source": [
        "\n",
        "coco_path = \"/content/DINO/COCODIR\"\n",
        "checkpoint_path = \"/content/drive/MyDrive/checkpoint0011_4scale24.pth\"\n",
        "eval_script_path = \"/content/DINO/scripts/DINO_eval.sh\"\n",
        "\n",
        "!bash {eval_script_path} {coco_path} {checkpoint_path}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cayt0rmCLEtu"
      },
      "source": [
        "# **Evaluation for 36 epoch**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "usBhuECVLJUG"
      },
      "outputs": [],
      "source": [
        "\n",
        "coco_path = \"/content/DINO/COCODIR\"\n",
        "checkpoint_path = \"/content/drive/MyDrive/checkpoint0011_4scale36.pth\"\n",
        "eval_script_path = \"/content/DINO/scripts/DINO_eval.sh\"\n",
        "\n",
        "!bash {eval_script_path} {coco_path} {checkpoint_path}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10FaeyZa1FRs"
      },
      "source": [
        "# **Average Precision (AP) values obtained from the validation set**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N65F8Ujq0_qX",
        "outputId": "42ec9a3b-131a-4f95-a993-fb15c02b84d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running evaluation for 12_epoch...\n",
            "Running evaluation for 24_epoch...\n",
            "Running evaluation for 36_epoch...\n",
            "          AP@[IoU=0.50:0.95]  AP@[IoU=0.50]  AP@[IoU=0.75]  AP@Small  AP@Large\n",
            "12_epoch               0.461          0.461          0.486     0.406     0.684\n",
            "24_epoch               0.461          0.461          0.486     0.406     0.684\n",
            "36_epoch               0.461          0.461          0.486     0.406     0.684\n"
          ]
        }
      ],
      "source": [
        "import subprocess\n",
        "import re\n",
        "\n",
        "\n",
        "def run_evaluation(coco_path, checkpoint_path, eval_script_path):\n",
        "    command = f\"bash {eval_script_path} {coco_path} {checkpoint_path}\"\n",
        "    result = subprocess.run(command, shell=True, capture_output=True, text=True)\n",
        "    return result.stdout\n",
        "\n",
        "\n",
        "def extract_ap_metrics(output):\n",
        "    ap_metrics = {}\n",
        "\n",
        "\n",
        "    ap_50_95 = re.search(r\"Average Precision.+IoU=0\\.50:0\\.95.+area=.+all.+maxDets=100.+?= (0\\.\\d+)\", output)\n",
        "    ap_50 = re.search(r\"Average Precision.+IoU=0\\.50.+area=.+all.+maxDets=100.+?= (0\\.\\d+)\", output)\n",
        "    ap_75 = re.search(r\"Average Precision.+IoU=0\\.75.+area=.+all.+maxDets=100.+?= (0\\.\\d+)\", output)\n",
        "    ap_small = re.search(r\"Average Precision.+IoU=0\\.50:0\\.95.+area=.+small.+maxDets=100.+?= (0\\.\\d+)\", output)\n",
        "    ap_medium = re.search(r\"Average Precision.+IoU=0\\.50:0\\.95.+area=.+medium.+maxDets=100.+?= (0\\.\\d+)\", output)\n",
        "    ap_large = re.search(r\"Average Precision.+IoU=0\\.50:0\\.95.+area=.+large.+maxDets=100.+?= (0\\.\\d+)\", output)\n",
        "\n",
        "    if ap_50_95: ap_metrics['AP@[IoU=0.50:0.95]'] = float(ap_50_95.group(1))\n",
        "    if ap_50: ap_metrics['AP@[IoU=0.50]'] = float(ap_50.group(1))\n",
        "    if ap_75: ap_metrics['AP@[IoU=0.75]'] = float(ap_75.group(1))\n",
        "    if ap_small: ap_metrics['AP@Small'] = float(ap_small.group(1))\n",
        "    if ap_medium: ap_metrics['AP@Medium'] = float(ap_medium.group(1))\n",
        "    if ap_large: ap_metrics['AP@Large'] = float(ap_large.group(1))\n",
        "\n",
        "    return ap_metrics\n",
        "\n",
        "\n",
        "coco_path = \"/content/DINO/COCODIR\"\n",
        "eval_script_path = \"/content/DINO/scripts/DINO_eval.sh\"\n",
        "\n",
        "\n",
        "checkpoints = {\n",
        "    \"12_epoch\": \"/content/drive/MyDrive/checkpoint0011_4scale.pth\",\n",
        "    \"24_epoch\": \"/content/drive/MyDrive/checkpoint0011_4scale24.pth\",\n",
        "    \"36_epoch\": \"/content/drive/MyDrive/checkpoint0011_4scale36.pth\"\n",
        "}\n",
        "\n",
        "\n",
        "ap_results = {}\n",
        "for epoch, checkpoint_path in checkpoints.items():\n",
        "    print(f\"Running evaluation for {epoch}...\")\n",
        "    output = run_evaluation(coco_path, checkpoint_path, eval_script_path)\n",
        "    ap_results[epoch] = extract_ap_metrics(output)\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame.from_dict(ap_results, orient='index')\n",
        "print(df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QM-EUARQ1LmT"
      },
      "outputs": [],
      "source": [
        "import os, sys\n",
        "import torch, json\n",
        "import numpy as np\n",
        "\n",
        "from main import build_model_main\n",
        "from util.slconfig import SLConfig\n",
        "from datasets import build_dataset\n",
        "from util.visualizer import COCOVisualizer\n",
        "from util import box_ops"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mp97fC1e1OV3",
        "outputId": "7568c783-b052-4350-8c08-d5363075398b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/DINO/models/dino/ops\n"
          ]
        }
      ],
      "source": [
        "%cd /content/DINO/models/dino/ops"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wtBhpbK61Q0k"
      },
      "outputs": [],
      "source": [
        "!python setup.py build_ext --inplace"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dkd-U8ul1TGl"
      },
      "outputs": [],
      "source": [
        "model_config_path = \"/content/DINO/config/DINO/DINO_4scale.py\"\n",
        "model_checkpoint_path = \"/content/drive/MyDrive/checkpoint0011_4scale.pth\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L7eg2JHH1Vak"
      },
      "outputs": [],
      "source": [
        "args = SLConfig.fromfile(model_config_path)\n",
        "args.device = 'cuda'\n",
        "model, criterion, postprocessors = build_model_main(args)\n",
        "checkpoint = torch.load(model_checkpoint_path, map_location='cpu')\n",
        "model.load_state_dict(checkpoint['model'])\n",
        "_ = model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A82h5Zfi1Yfd"
      },
      "outputs": [],
      "source": [
        "# load coco names\n",
        "with open('/content/DINO/util/coco_id2name.json') as f:\n",
        "    id2name = json.load(f)\n",
        "    id2name = {int(k):v for k,v in id2name.items()}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SBsgvIFw1dDs"
      },
      "source": [
        "**Visualize images from a dataloader**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w6aZw8GE1cIj",
        "outputId": "e62299ea-5e1a-498f-ee61-99c581f9f54f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data_aug_params: {\n",
            "  \"scales\": [\n",
            "    480,\n",
            "    512,\n",
            "    544,\n",
            "    576,\n",
            "    608,\n",
            "    640,\n",
            "    672,\n",
            "    704,\n",
            "    736,\n",
            "    768,\n",
            "    800\n",
            "  ],\n",
            "  \"max_size\": 1333,\n",
            "  \"scales2_resize\": [\n",
            "    400,\n",
            "    500,\n",
            "    600\n",
            "  ],\n",
            "  \"scales2_crop\": [\n",
            "    384,\n",
            "    600\n",
            "  ]\n",
            "}\n",
            "loading annotations into memory...\n",
            "Done (t=0.00s)\n",
            "creating index...\n",
            "index created!\n"
          ]
        }
      ],
      "source": [
        "\n",
        "args.dataset_file = 'coco'\n",
        "args.coco_path = \"/content/DINO/COCODIR\"\n",
        "args.fix_size = False\n",
        "\n",
        "# Build the validation dataset\n",
        "dataset_val = build_dataset(image_set='val', args=args)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8xLxvAcu1g_m"
      },
      "source": [
        "**Get an Example and Visualize it**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MV_op3kP1fO6"
      },
      "outputs": [],
      "source": [
        "image, targets = dataset_val[9]"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "M0TJZgFQq_mk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lK8gFpoR8Alt"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4JoQJEYz1v3l"
      },
      "source": [
        "**The image being generated after running the below cell is the actual result**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RpaUFK1n1mIn"
      },
      "outputs": [],
      "source": [
        "# Define id2name for the single class\n",
        "id2name = {1: \"person\"}  # Assuming class ID for \"person\" is 1\n",
        "\n",
        "# Build gt_dict for visualization\n",
        "box_label = [id2name[int(item)] for item in targets['labels']]\n",
        "gt_dict = {\n",
        "    'boxes': targets['boxes'],\n",
        "    'image_id': targets['image_id'],\n",
        "    'size': targets['size'],\n",
        "    'box_label': box_label,\n",
        "}\n",
        "\n",
        "# Visualize\n",
        "vslzr = COCOVisualizer()\n",
        "vslzr.visualize(image, gt_dict, savedir=None)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vM-Nju591_8b"
      },
      "source": [
        "**Visualize Model Predictions**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EU-cJGLG1y54",
        "outputId": "ecb7c159-dec4-4d6f-ae1c-bb1035acef81"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/functional.py:534: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3595.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
          ]
        }
      ],
      "source": [
        "output = model.cuda()(image[None].cuda())\n",
        "output = postprocessors['bbox'](output, torch.Tensor([[1.0, 1.0]]).cuda())[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0EMyTWXy1z5K"
      },
      "outputs": [],
      "source": [
        "thershold = 0.3\n",
        "\n",
        "scores = output['scores']\n",
        "labels = output['labels']\n",
        "boxes = box_ops.box_xyxy_to_cxcywh(output['boxes'])\n",
        "select_mask = scores > thershold"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8PUTT7Q2GvI"
      },
      "source": [
        "**The image being generated after running the below cell is the predcited result**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w4EyVPl112nd"
      },
      "outputs": [],
      "source": [
        "\n",
        "filtered_labels = labels[select_mask]\n",
        "filtered_boxes = boxes[select_mask]\n",
        "id2name = {1: \"person\"}\n",
        "box_label = [id2name[int(item)] for item in filtered_labels if int(item) in id2name]\n",
        "pred_dict = {\n",
        "    'boxes': filtered_boxes,\n",
        "    'size': targets['size'],\n",
        "    'box_label': box_label\n",
        "}\n",
        "\n",
        "vslzr.visualize(image, pred_dict, savedir=None)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7OeJ20RYBzyK"
      },
      "source": [
        "**Model Evaluation: Ground Truth vs. Predicted Bounding Box Visualization (Epochs 12, 24, 36)-Analysis on Pre-trained model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BFpsszNt8Q8t"
      },
      "outputs": [],
      "source": [
        "import os, torch, json\n",
        "from main import build_model_main\n",
        "from util.slconfig import SLConfig\n",
        "from datasets import build_dataset\n",
        "from util.visualizer import COCOVisualizer\n",
        "from util import box_ops\n",
        "\n",
        "# Common configuration and dataset loading\n",
        "model_config_path = \"/content/DINO/config/DINO/DINO_4scale.py\"\n",
        "args = SLConfig.fromfile(model_config_path)\n",
        "args.device = 'cuda'\n",
        "args.dataset_file = 'coco'\n",
        "args.coco_path = \"/content/DINO/COCODIR\"\n",
        "args.fix_size = False\n",
        "\n",
        "# Build the validation dataset\n",
        "dataset_val = build_dataset(image_set='val', args=args)\n",
        "\n",
        "# COCO visualizer instance\n",
        "vslzr = COCOVisualizer()\n",
        "\n",
        "# Single class dictionary for \"person\"\n",
        "id2name = {1: \"person\"}\n",
        "\n",
        "# Function to visualize ground truth (actual) bounding boxes\n",
        "def visualize_actual(image, targets):\n",
        "    # Ground Truth Visualization (Actual)\n",
        "    gt_box_label = [id2name[int(item)] for item in targets['labels']]\n",
        "    gt_dict = {\n",
        "        'boxes': targets['boxes'],\n",
        "        'image_id': targets['image_id'],\n",
        "        'size': targets['size'],\n",
        "        'box_label': gt_box_label,\n",
        "    }\n",
        "\n",
        "    # Visualize the actual image with ground truth boxes\n",
        "    print(\"Visualizing Actual Bounding Boxes\")\n",
        "    vslzr.visualize(image, gt_dict, savedir=None)\n",
        "\n",
        "# Function to visualize model predictions (predicted)\n",
        "def visualize_predicted(image, output, targets, threshold=0.3):\n",
        "    # Model Prediction Visualization\n",
        "    scores = output['scores']\n",
        "    labels = output['labels']\n",
        "    boxes = box_ops.box_xyxy_to_cxcywh(output['boxes'])\n",
        "    select_mask = scores > threshold\n",
        "    filtered_labels = labels[select_mask]\n",
        "    filtered_boxes = boxes[select_mask]\n",
        "\n",
        "    # Create a dictionary for the predicted boxes\n",
        "    pred_box_label = [id2name[int(item)] for item in filtered_labels if int(item) in id2name]\n",
        "    pred_dict = {\n",
        "        'boxes': filtered_boxes,\n",
        "        'size': targets['size'],\n",
        "        'box_label': pred_box_label\n",
        "    }\n",
        "\n",
        "    # Visualize the predicted image with predicted boxes\n",
        "    print(\"Visualizing Predicted Bounding Boxes\")\n",
        "    vslzr.visualize(image, pred_dict, savedir=None)\n",
        "\n",
        "# Prepare function to load a model and predict results for specific images (image 0 and image 7)\n",
        "def load_model_and_predict(checkpoint_path, image_indices=[0, 7]):\n",
        "    # Load model and checkpoint\n",
        "    model, criterion, postprocessors = build_model_main(args)\n",
        "    checkpoint = torch.load(checkpoint_path, map_location='cpu')\n",
        "    model.load_state_dict(checkpoint['model'])\n",
        "    _ = model.eval()\n",
        "\n",
        "    # Get predictions and visualize for the specified images\n",
        "    for i in image_indices:\n",
        "        image, targets = dataset_val[i]\n",
        "\n",
        "        # Visualize the actual (ground truth) bounding boxes\n",
        "        visualize_actual(image, targets)\n",
        "\n",
        "        # Get model predictions\n",
        "        output = model.cuda()(image[None].cuda())\n",
        "        output = postprocessors['bbox'](output, torch.Tensor([[1.0, 1.0]]).cuda())[0]\n",
        "\n",
        "        # Visualize the predicted bounding boxes\n",
        "        visualize_predicted(image, output, targets)\n",
        "\n",
        "# Run prediction and visualization for different checkpoints and images 0, 7\n",
        "print(\"Running for 12 epoch checkpoint:\")\n",
        "load_model_and_predict(\"/content/drive/MyDrive/checkpoint0011_4scale.pth\")\n",
        "\n",
        "print(\"Running for 24 epoch checkpoint:\")\n",
        "load_model_and_predict(\"/content/drive/MyDrive/checkpoint0011_4scale24.pth\")\n",
        "\n",
        "print(\"Running for 36 epoch checkpoint:\")\n",
        "load_model_and_predict(\"/content/drive/MyDrive/checkpoint0011_4scale36.pth\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LolivvGuqtJX"
      },
      "source": [
        "**The images above illustrate the performance of the pretrained models at various epoch checkpoints. Given the simplicity and lower crowd density of the scenes, the model successfully detects pedestrians**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w89vikmSqv0e"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HUg5eopMqwkO"
      },
      "source": [
        "**Instances where the model failed to accurately detect objects occurred in more complex images, often containing multiple obstacles. In these cases, the model misclassified or missed objects due to the increased scene complexity**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i4bvqU0HDVLm",
        "outputId": "e98767c3-a3c8-450b-8625-0607cb762ae3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data_aug_params: {\n",
            "  \"scales\": [\n",
            "    480,\n",
            "    512,\n",
            "    544,\n",
            "    576,\n",
            "    608,\n",
            "    640,\n",
            "    672,\n",
            "    704,\n",
            "    736,\n",
            "    768,\n",
            "    800\n",
            "  ],\n",
            "  \"max_size\": 1333,\n",
            "  \"scales2_resize\": [\n",
            "    400,\n",
            "    500,\n",
            "    600\n",
            "  ],\n",
            "  \"scales2_crop\": [\n",
            "    384,\n",
            "    600\n",
            "  ]\n",
            "}\n",
            "loading annotations into memory...\n",
            "Done (t=0.00s)\n",
            "creating index...\n",
            "index created!\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "from main import build_model_main\n",
        "from util.slconfig import SLConfig\n",
        "from datasets import build_dataset\n",
        "from util.visualizer import COCOVisualizer\n",
        "from util import box_ops\n",
        "\n",
        "# Common configuration and dataset loading\n",
        "model_config_path = \"/content/DINO/config/DINO/DINO_4scale.py\"\n",
        "args = SLConfig.fromfile(model_config_path)\n",
        "args.device = 'cuda'\n",
        "args.dataset_file = 'coco'\n",
        "args.coco_path = \"/content/DINO/COCODIR\"\n",
        "args.fix_size = False\n",
        "\n",
        "# Build the validation dataset\n",
        "dataset_val = build_dataset(image_set='val', args=args)\n",
        "\n",
        "# COCO visualizer instance\n",
        "vslzr = COCOVisualizer()\n",
        "\n",
        "# Single class dictionary for \"person\"\n",
        "id2name = {1: \"person\"}\n",
        "\n",
        "# Function to visualize ground truth (actual) bounding boxes\n",
        "def visualize_actual(image, targets, epoch):\n",
        "    gt_box_label = [id2name[int(item)] for item in targets['labels']]\n",
        "    gt_dict = {\n",
        "        'boxes': targets['boxes'],\n",
        "        'image_id': targets['image_id'],\n",
        "        'size': targets['size'],\n",
        "        'box_label': gt_box_label,\n",
        "    }\n",
        "    print(\"Visualizing Actual Bounding Boxes\")\n",
        "    vslzr.visualize(image, gt_dict, savedir=f'/content/results/epoch_{epoch}_actual.jpg')\n",
        "\n",
        "# Function to visualize model predictions (predicted)\n",
        "def visualize_predicted(image, output, targets, epoch, threshold=0.3):\n",
        "    scores = output['scores']\n",
        "    labels = output['labels']\n",
        "    boxes = box_ops.box_xyxy_to_cxcywh(output['boxes'])\n",
        "    select_mask = scores > threshold\n",
        "    filtered_labels = labels[select_mask]\n",
        "    filtered_boxes = boxes[select_mask]\n",
        "\n",
        "    pred_box_label = [id2name[int(item)] for item in filtered_labels if int(item) in id2name]\n",
        "    pred_dict = {\n",
        "        'boxes': filtered_boxes,\n",
        "        'size': targets['size'],\n",
        "        'box_label': pred_box_label\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        assert len(pred_dict['box_label']) == targets['boxes'].shape[0], \\\n",
        "            f\"{len(pred_dict['box_label'])} = {targets['boxes'].shape[0]}, Wrong prediction\"\n",
        "    except AssertionError:\n",
        "        print(\"Wrong Prediction detected: the number of predicted boxes does not match ground truth.\")\n",
        "        pred_dict['box_label'] = ['Wrong Prediction'] * len(filtered_boxes)\n",
        "\n",
        "    print(\"Visualizing Predicted Bounding Boxes\")\n",
        "    vslzr.visualize(image, pred_dict, savedir=f'/content/results/epoch_{epoch}_predicted.jpg')\n",
        "\n",
        "# Function to load a model and predict results for specific images\n",
        "def load_model_and_predict(checkpoint_path, image_indices=[6], epoch=None):\n",
        "    try:\n",
        "        model, criterion, postprocessors = build_model_main(args)\n",
        "        checkpoint = torch.load(checkpoint_path, map_location='cpu')\n",
        "        model.load_state_dict(checkpoint['model'])\n",
        "        model.eval()\n",
        "\n",
        "        for i in image_indices:\n",
        "            image, targets = dataset_val[i]\n",
        "\n",
        "            # Visualize the actual (ground truth) bounding boxes\n",
        "            visualize_actual(image, targets, epoch)\n",
        "\n",
        "            # Get model predictions\n",
        "            output = model.cuda()(image[None].cuda())\n",
        "            output = postprocessors['bbox'](output, torch.Tensor([[1.0, 1.0]]).cuda())[0]\n",
        "\n",
        "            # Visualize the predicted bounding boxes\n",
        "            visualize_predicted(image, output, targets, epoch)\n",
        "    except Exception as e:\n",
        "        print(f\"Error during loading or prediction for checkpoint {checkpoint_path}: {e}\")\n",
        "\n",
        "# Create a directory to save results if it doesn't exist\n",
        "os.makedirs('/content/results', exist_ok=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fWaDE-A8Dfam"
      },
      "source": [
        "*Run the above code before running tests for different epoch checkpoints*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wlTLoXgYq2qw"
      },
      "source": [
        "# **Load and Predict for 12th Epoch Checkpoint**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u8TZi-pYq4tp"
      },
      "outputs": [],
      "source": [
        "# Run prediction for 12th Epoch Checkpoint\n",
        "print(\"Running for 12th Epoch Checkpoint:\")\n",
        "load_model_and_predict(\"/content/drive/MyDrive/checkpoint0011_4scale.pth\", epoch=12)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K5BiO-L7q7qO"
      },
      "source": [
        "# **Load and Predict for 24th Epoch Checkpoint**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vOXS5LDiq9qf"
      },
      "outputs": [],
      "source": [
        "# Run prediction for 24th Epoch Checkpoint\n",
        "print(\"Running for 24th Epoch Checkpoint:\")\n",
        "load_model_and_predict(\"/content/drive/MyDrive/checkpoint0011_4scale24.pth\", epoch=24)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qwaUgZgPq_XP"
      },
      "source": [
        "# **Load and Predict for 36th Epoch Checkpoint**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l0BTMucWrB4y"
      },
      "outputs": [],
      "source": [
        "print(\"Running for 36th Epoch Checkpoint:\")\n",
        "load_model_and_predict(\"/content/drive/MyDrive/checkpoint0011_4scale36.pth\", epoch=36)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8zCxFCharJcF"
      },
      "source": [
        "Failure in accurate Object detection\n",
        "\n",
        "\n",
        "\n",
        "1.  Despite different epoch checkpoints, the pre-trained model struggles with pedestrian detection.\n",
        "2.   Over 80% of the dataset exhibits in-correct detections.\n",
        "3. Complex scenes hinder accurate identification\n",
        "4. Identifies extra classes\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-uKYGXr4ra2R"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZUjyI9U-77hV"
      },
      "source": [
        "\n",
        "\n",
        "1.   **checkpoint.pth** : performs best on simple images\n",
        "2.   **checkpoint12.pth,checkpoint24.pth,checkpoint36.pth** :performs best on complex images, with light varying and minute complex scenes followed by other checkpoints\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HXleEByXwxGK"
      },
      "source": [
        "# **Testing the model with no extra augmentation techniques**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PsFe3W1Qw27K",
        "outputId": "b6fc4bc8-9b1e-43d1-c87b-c14e021e900f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/DINO\n"
          ]
        }
      ],
      "source": [
        "%cd /content/DINO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "crARgrwlw4vC"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "import datasets.transforms as T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mrhtn9d4w7FA"
      },
      "outputs": [],
      "source": [
        "image = Image.open(\"/content/DINO/16065.jpg\").convert(\"RGB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fETnb5mEw-xU"
      },
      "outputs": [],
      "source": [
        "transform = T.Compose([\n",
        "    T.RandomResize([800], max_size=1333),\n",
        "    T.ToTensor(),\n",
        "    T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "image, _ = transform(image, None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Enx3gyBxBYS"
      },
      "outputs": [],
      "source": [
        "model_config_path = \"config/DINO/DINO_4scale.py\"\n",
        "model_checkpoint_path = \"/content/drive/MyDrive/checkpoint.pth\"#/content/drive/MyDrive/checkpoint.pth\n",
        "#/content/DINO/logs/DINO/R50-MS4/checkpoint.pth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4-D22qHxxFNM",
        "outputId": "9285817b-b141-4012-a996-f2c11ffcfe50"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-38-72ca36f16c8d>:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(model_checkpoint_path, map_location='cpu')\n"
          ]
        }
      ],
      "source": [
        "args = SLConfig.fromfile(model_config_path)\n",
        "args.device = 'cuda'\n",
        "model, criterion, postprocessors = build_model_main(args)\n",
        "checkpoint = torch.load(model_checkpoint_path, map_location='cpu')\n",
        "model.load_state_dict(checkpoint['model'])\n",
        "_ = model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TL-SLVvQxHYq"
      },
      "outputs": [],
      "source": [
        "output = model.cuda()(image[None].cuda())\n",
        "output = postprocessors['bbox'](output, torch.Tensor([[1.0, 1.0]]).cuda())[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fqze1nqFxH1-"
      },
      "outputs": [],
      "source": [
        "thershold = 0.3 # set a thershold\n",
        "\n",
        "vslzr = COCOVisualizer()\n",
        "\n",
        "scores = output['scores']\n",
        "labels = output['labels']\n",
        "boxes = box_ops.box_xyxy_to_cxcywh(output['boxes'])\n",
        "select_mask = scores > thershold\n",
        "\n",
        "box_label = [id2name[int(item)] for item in labels[select_mask]]\n",
        "pred_dict = {\n",
        "    'boxes': boxes[select_mask],\n",
        "    'size': torch.Tensor([image.shape[1], image.shape[2]]),\n",
        "    'box_label': box_label\n",
        "}\n",
        "vslzr.visualize(image, pred_dict, savedir=None, dpi=100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ESd2eEGydJ9v"
      },
      "source": [
        "# **24 epoch checkpoint with no augmentation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g7i85OpgZKyX",
        "outputId": "670b9c95-7f0b-4c79-c006-b347d60f3093"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/DINO\n"
          ]
        }
      ],
      "source": [
        "%cd /content/DINO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cnp-ZNZfZMRs"
      },
      "outputs": [],
      "source": [
        "!bash /content/DINO/scripts/DINO_train.sh /content/DINO/COCODIR \\\n",
        "--pretrain_model_path /content/drive/MyDrive/checkpoint0011_4scale24.pth \\\n",
        "--finetune_ignore label_enc.weight class_embed | tee /content/DINO/results/train_log.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Frh9xSPXbLGg"
      },
      "outputs": [],
      "source": [
        "!python main.py \\\n",
        "    --config_file config/DINO/DINO_4scale.py \\\n",
        "    --output_dir /content/DINO/results \\\n",
        "    --pretrain_model_path /content/DINO/logs/DINO/R50-MS4/checkpoint.pth \\\n",
        "    --coco_path /content/DINO/COCODIR \\\n",
        "    --eval \\\n",
        "    --options dn_scalar=100 embed_init_tgt=TRUE \\\n",
        "    dn_label_coef=1.0 dn_bbox_coef=1.0 use_ema=False \\\n",
        "    dn_box_noise_scale=1.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eea5A0VtSuZg"
      },
      "source": [
        "# **Models below are trained with extra data augmentations**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sx15CZ-KreSh"
      },
      "source": [
        "# **Fine-Tuning the Pre-Trained Model 12 epoch checkpoint**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4dJ5FfuUrgOo",
        "outputId": "b50eebc7-582f-4254-f3ea-43b3047dbc97"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/DINO\n"
          ]
        }
      ],
      "source": [
        "%cd /content/DINO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5L46Bcq-rhaJ"
      },
      "outputs": [],
      "source": [
        "!bash /content/DINO/scripts/DINO_train.sh /content/DINO/COCODIR \\\n",
        "--pretrain_model_path /content/drive/MyDrive/checkpoint0011_4scale.pth \\\n",
        "--finetune_ignore label_enc.weight class_embed | tee /content/DINO/results/train_log.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n-a0vdP184V6"
      },
      "source": [
        "# **Re-Evaluating the results on the validation-set**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9n1-hw_I84uK"
      },
      "outputs": [],
      "source": [
        "!python main.py \\\n",
        "    --config_file config/DINO/DINO_4scale.py \\\n",
        "    --output_dir /content/DINO/results \\\n",
        "    --pretrain_model_path /content/drive/MyDrive/checkpoint12.pth \\\n",
        "    --coco_path /content/DINO/COCODIR \\\n",
        "    --eval \\\n",
        "    --options dn_scalar=100 embed_init_tgt=TRUE \\\n",
        "    dn_label_coef=1.0 dn_bbox_coef=1.0 use_ema=False \\\n",
        "    dn_box_noise_scale=1.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QViR6B8z9WMX"
      },
      "source": [
        "# **Testing the fine-tuned model on custom images/ Validation images**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M9W0OXYK9YWs"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "import datasets.transforms as T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ewFgQ0vS9apb"
      },
      "outputs": [],
      "source": [
        "image = Image.open(\"/content/DINO/16154.jpg\").convert(\"RGB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QIrpihTB9cUE"
      },
      "outputs": [],
      "source": [
        "transform = T.Compose([\n",
        "    T.RandomResize([800], max_size=1333),\n",
        "    T.ToTensor(),\n",
        "    T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "image, _ = transform(image, None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K7XnSnet9lLd"
      },
      "outputs": [],
      "source": [
        "model_config_path = \"config/DINO/DINO_4scale.py\"\n",
        "model_checkpoint_path = \"/content/DINO/logs/DINO/R50-MS4/checkpoint.pth\"#/content/drive/MyDrive/checkpoint.pth\n",
        "#/content/DINO/logs/DINO/R50-MS4/checkpoint.pth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JpYWSAHQ9nus",
        "outputId": "4b26fd67-296a-463a-d1f8-4aed65b1a0ac"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-83-72ca36f16c8d>:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(model_checkpoint_path, map_location='cpu')\n"
          ]
        }
      ],
      "source": [
        "args = SLConfig.fromfile(model_config_path)\n",
        "args.device = 'cuda'\n",
        "model, criterion, postprocessors = build_model_main(args)\n",
        "checkpoint = torch.load(model_checkpoint_path, map_location='cpu')\n",
        "model.load_state_dict(checkpoint['model'])\n",
        "_ = model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cv-fzYS99puO"
      },
      "outputs": [],
      "source": [
        "output = model.cuda()(image[None].cuda())\n",
        "output = postprocessors['bbox'](output, torch.Tensor([[1.0, 1.0]]).cuda())[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W-4gPAXL9r4k"
      },
      "outputs": [],
      "source": [
        "thershold = 0.3 # set a thershold\n",
        "\n",
        "vslzr = COCOVisualizer()\n",
        "\n",
        "scores = output['scores']\n",
        "labels = output['labels']\n",
        "boxes = box_ops.box_xyxy_to_cxcywh(output['boxes'])\n",
        "select_mask = scores > thershold\n",
        "\n",
        "box_label = [id2name[int(item)] for item in labels[select_mask]]\n",
        "pred_dict = {\n",
        "    'boxes': boxes[select_mask],\n",
        "    'size': torch.Tensor([image.shape[1], image.shape[2]]),\n",
        "    'box_label': box_label\n",
        "}\n",
        "vslzr.visualize(image, pred_dict, savedir=None, dpi=100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PE0epTD7D73v"
      },
      "source": [
        "# **Fine-Tuning pre trained 4- Scale 24 epoch checkpoint**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2dlYR19jEENa",
        "outputId": "071bbee9-f7d8-4d63-b7f8-92be665cf5f2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/DINO\n"
          ]
        }
      ],
      "source": [
        "%cd /content/DINO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "or1KsA6HEJsI"
      },
      "outputs": [],
      "source": [
        "!bash /content/DINO/scripts/DINO_train.sh /content/DINO/COCODIR \\\n",
        "--pretrain_model_path /content/drive/MyDrive/checkpoint0011_4scale24.pth \\\n",
        "--finetune_ignore label_enc.weight class_embed | tee /content/DINO/results/train_log.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CSkl70qKEP66"
      },
      "source": [
        "**Re-Evaluating the results on the validation-set(24)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A5KvXXMZEPWu"
      },
      "outputs": [],
      "source": [
        "!python main.py \\\n",
        "    --config_file config/DINO/DINO_4scale.py \\\n",
        "    --output_dir /content/DINO/results \\\n",
        "    --pretrain_model_path /content/DINO/logs/DINO/R50-MS4/checkpoint.pth \\\n",
        "    --coco_path /content/DINO/COCODIR \\\n",
        "    --eval \\\n",
        "    --options dn_scalar=100 embed_init_tgt=TRUE \\\n",
        "    dn_label_coef=1.0 dn_bbox_coef=1.0 use_ema=False \\\n",
        "    dn_box_noise_scale=1.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qoo9Cy3rGR9g"
      },
      "source": [
        "# **Testing the fine-tuned model on custom images/ Validation images for 24 epoch ckpt**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LmHeMjEBGVgK"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "import datasets.transforms as T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "llIyV2VjGbbX"
      },
      "outputs": [],
      "source": [
        "image = Image.open(\"/content/DINO/9260.jpg\").convert(\"RGB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b9w21EASGcCi"
      },
      "outputs": [],
      "source": [
        "transform = T.Compose([\n",
        "    T.RandomResize([800], max_size=1333),\n",
        "    T.ToTensor(),\n",
        "    T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "image, _ = transform(image, None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G2n8oL4lK1UJ"
      },
      "outputs": [],
      "source": [
        "model_config_path = \"config/DINO/DINO_4scale.py\"\n",
        "model_checkpoint_path = \"/content/drive/MyDrive/checkpoint.pth\"\n",
        "#/content/drive/MyDrive/checkpoint.pth\n",
        "#/content/drive/MyDrive/checkpoint12.pth\n",
        "#/content/DINO/logs/DINO/R50-MS4/checkpoint.pth\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "oHZt_n31GoBM"
      },
      "outputs": [],
      "source": [
        "args = SLConfig.fromfile(model_config_path)\n",
        "args.device = 'cuda'\n",
        "model, criterion, postprocessors = build_model_main(args)\n",
        "checkpoint = torch.load(model_checkpoint_path, map_location='cpu')\n",
        "model.load_state_dict(checkpoint['model'])\n",
        "_ = model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mSmngbTtGpEP"
      },
      "outputs": [],
      "source": [
        "output = model.cuda()(image[None].cuda())\n",
        "output = postprocessors['bbox'](output, torch.Tensor([[1.0, 1.0]]).cuda())[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zFJb7-CqGqxY"
      },
      "outputs": [],
      "source": [
        "thershold = 0.3 # set a thershold\n",
        "\n",
        "vslzr = COCOVisualizer()\n",
        "\n",
        "scores = output['scores']\n",
        "labels = output['labels']\n",
        "boxes = box_ops.box_xyxy_to_cxcywh(output['boxes'])\n",
        "select_mask = scores > thershold\n",
        "\n",
        "box_label = [id2name[int(item)] for item in labels[select_mask]]\n",
        "pred_dict = {\n",
        "    'boxes': boxes[select_mask],\n",
        "    'size': torch.Tensor([image.shape[1], image.shape[2]]),\n",
        "    'box_label': box_label\n",
        "}\n",
        "vslzr.visualize(image, pred_dict, savedir=None, dpi=100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XVfHL-y2Oe04"
      },
      "source": [
        "# **Fine-Tuning pre trained 4- Scale 36 epoch checkpoint**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XffUHKNXOguM",
        "outputId": "054aee6f-1b90-426d-b1ee-7188e9765ebd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/DINO\n"
          ]
        }
      ],
      "source": [
        "%cd /content/DINO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UPEoBotXOk2J"
      },
      "outputs": [],
      "source": [
        "!bash /content/DINO/scripts/DINO_train.sh /content/DINO/COCODIR \\\n",
        "--pretrain_model_path /content/drive/MyDrive/checkpoint0011_4scale36.pth \\\n",
        "--finetune_ignore label_enc.weight class_embed | tee /content/DINO/results/train_log.txt\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZ-5-zyIOnPr"
      },
      "source": [
        "**Re-Evaluating the results on the validation-set**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OjGJ8fKKOn65"
      },
      "outputs": [],
      "source": [
        "!python main.py \\\n",
        "    --config_file config/DINO/DINO_4scale.py \\\n",
        "    --output_dir /content/DINO/results \\\n",
        "    --pretrain_model_path /content/DINO/logs/DINO/R50-MS4/checkpoint.pth \\\n",
        "    --coco_path /content/DINO/COCODIR \\\n",
        "    --eval \\\n",
        "    --options dn_scalar=100 embed_init_tgt=TRUE \\\n",
        "    dn_label_coef=1.0 dn_bbox_coef=1.0 use_ema=False \\\n",
        "    dn_box_noise_scale=1.0\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LwVziG4JOp6d"
      },
      "source": [
        "# **Testing the fine-tuned model on custom images/ Validation images 36**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9CwAdS9cOuRB"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "import datasets.transforms as T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DYZ2hHieOvT8"
      },
      "outputs": [],
      "source": [
        "image = Image.open(\"/content/DINO/17137.jpg\").convert(\"RGB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KisHKgyxOwuD"
      },
      "outputs": [],
      "source": [
        "transform = T.Compose([\n",
        "    T.RandomResize([800], max_size=1333),\n",
        "    T.ToTensor(),\n",
        "    T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "image, _ = transform(image, None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kTk-JPlJOxqx"
      },
      "outputs": [],
      "source": [
        "model_config_path = \"config/DINO/DINO_4scale.py\"\n",
        "model_checkpoint_path = \"/content/DINO/results/fused_model_checkpoint.pth\"\n",
        "#/content/drive/MyDrive/checkpoint.pth\n",
        "#/content/drive/MyDrive/checkpoint12.pth\n",
        "#/content/drive/MyDrive/checkpoint24.pth\n",
        "#/content/drive/MyDrive/checkpoint36.pth\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uM2W7WDAOyw2",
        "outputId": "da11ba25-fbe4-4b9e-ddee-749c64e283d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-59-72ca36f16c8d>:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(model_checkpoint_path, map_location='cpu')\n"
          ]
        }
      ],
      "source": [
        "args = SLConfig.fromfile(model_config_path)\n",
        "args.device = 'cuda'\n",
        "model, criterion, postprocessors = build_model_main(args)\n",
        "checkpoint = torch.load(model_checkpoint_path, map_location='cpu')\n",
        "model.load_state_dict(checkpoint['model'])\n",
        "_ = model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HISUQZlHOzvp"
      },
      "outputs": [],
      "source": [
        "output = model.cuda()(image[None].cuda())\n",
        "output = postprocessors['bbox'](output, torch.Tensor([[1.0, 1.0]]).cuda())[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j5u7mL8gO0kU"
      },
      "outputs": [],
      "source": [
        "thershold = 0.3 # set a thershold\n",
        "\n",
        "vslzr = COCOVisualizer()\n",
        "\n",
        "scores = output['scores']\n",
        "labels = output['labels']\n",
        "boxes = box_ops.box_xyxy_to_cxcywh(output['boxes'])\n",
        "select_mask = scores > thershold\n",
        "\n",
        "box_label = [id2name[int(item)] for item in labels[select_mask]]\n",
        "pred_dict = {\n",
        "    'boxes': boxes[select_mask],\n",
        "    'size': torch.Tensor([image.shape[1], image.shape[2]]),\n",
        "    'box_label': box_label\n",
        "}\n",
        "vslzr.visualize(image, pred_dict, savedir=None, dpi=100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1P54chGNWnpx"
      },
      "source": [
        "# **Visualizing each fine tuned model based on different epoch setting to compare**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E_NxdcWwnO65"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "import datasets.transforms as T\n",
        "\n",
        "\n",
        "# Define the image transformation\n",
        "transform = T.Compose([\n",
        "    T.RandomResize([800], max_size=1333),\n",
        "    T.ToTensor(),\n",
        "    T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "\n",
        "image = Image.open(\"/content/DINO/6458.jpg\").convert(\"RGB\")\n",
        "image, _ = transform(image, None)\n",
        "\n",
        "\n",
        "checkpoints = {\n",
        "\n",
        "    \"checkpoint.pth\": \"/content/drive/MyDrive/checkpoint.pth\",\n",
        "    \"checkpoint24.pth\":\"/content/drive/MyDrive/checkpoint24norm.pth\",\n",
        "    \"checkpoint36.pth\":\"/content/drive/MyDrive/checkpoint36norm.pth\",\n",
        "    \"checkpoint12withaug.pth\": \"/content/drive/MyDrive/checkpoint12.pth\",\n",
        "    \"checkpoint24withaug.pth\": \"/content/drive/MyDrive/checkpoint24.pth\",\n",
        "    \"checkpoint36withaug.pth\": \"/content/drive/MyDrive/checkpoint36.pth\"\n",
        "}\n",
        "\n",
        "\n",
        "vslzr = COCOVisualizer()\n",
        "\n",
        "\n",
        "for title, checkpoint_path in checkpoints.items():\n",
        "    model_config_path = \"config/DINO/DINO_4scale.py\"\n",
        "    args = SLConfig.fromfile(model_config_path)\n",
        "    args.device = 'cuda'\n",
        "    model, criterion, postprocessors = build_model_main(args)\n",
        "    checkpoint = torch.load(checkpoint_path, map_location='cpu')\n",
        "    model.load_state_dict(checkpoint['model'])\n",
        "    _ = model.eval()\n",
        "\n",
        "    output = model.cuda()(image[None].cuda())\n",
        "    output = postprocessors['bbox'](output, torch.Tensor([[1.0, 1.0]]).cuda())[0]\n",
        "    threshold = 0.3  # set a threshold\n",
        "    scores = output['scores']\n",
        "    labels = output['labels']\n",
        "    boxes = box_ops.box_xyxy_to_cxcywh(output['boxes'])\n",
        "    select_mask = scores > threshold\n",
        "    box_label = [id2name[int(item)] for item in labels[select_mask]]\n",
        "    pred_dict = {\n",
        "        'boxes': boxes[select_mask],\n",
        "        'size': torch.Tensor([image.shape[1], image.shape[2]]),\n",
        "        'box_label': box_label\n",
        "    }\n",
        "\n",
        "    # Visualize the results\n",
        "    print(f\"{title}:\")\n",
        "    vslzr.visualize(image, pred_dict, savedir=None, dpi=100)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*The average precision values were extracted frome valuation scores extracted after the Fine Tuning process *"
      ],
      "metadata": {
        "id": "2H7zfWq_tPV5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Data from the table\n",
        "models = [\n",
        "    \"Checkpoint 12 with Aug\",\n",
        "    \"Checkpoint 36 with Aug\",\n",
        "    \"Checkpoint 36 no Aug\",\n",
        "    \"Checkpoint 24 no Aug\",\n",
        "    \"Checkpoint 12 no Aug\",\n",
        "    \"Checkpoint 24 with Aug\"\n",
        "]\n",
        "medium_ap = [0.361, 0.342, 0.328, 0.321, 0.316, 0.312]\n",
        "ap_50 = [0.694, 0.670, 0.645, 0.633, 0.610, 0.625]\n",
        "ar = [0.610, 0.592, 0.578, 0.585, 0.540, 0.563]\n",
        "\n",
        "# Set up the bar width and position\n",
        "bar_width = 0.25\n",
        "x = np.arange(len(models))\n",
        "\n",
        "# Create the bar chart\n",
        "fig, ax = plt.subplots(figsize=(12, 6))\n",
        "bars1 = ax.bar(x - bar_width, medium_ap, bar_width, label=\"Medium AP @ 0.50:0.95\")\n",
        "bars2 = ax.bar(x, ap_50, bar_width, label=\"AP @ 0.50\")\n",
        "bars3 = ax.bar(x + bar_width, ar, bar_width, label=\"AR\")\n",
        "\n",
        "# Add labels and title\n",
        "ax.set_xlabel(\"Model Checkpoints\")\n",
        "ax.set_ylabel(\"Metrics\")\n",
        "ax.set_title(\"Model Performance on Medium AP, AP @ 0.50, and AR (Person Detection)\")\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(models, rotation=45, ha=\"right\")\n",
        "ax.legend()\n",
        "\n",
        "# Display values on each bar for clarity\n",
        "def add_values(bars):\n",
        "    for bar in bars:\n",
        "        height = bar.get_height()\n",
        "        ax.annotate(f\"{height:.3f}\",\n",
        "                    xy=(bar.get_x() + bar.get_width() / 2, height),\n",
        "                    xytext=(0, 3),  # Offset the text a bit\n",
        "                    textcoords=\"offset points\",\n",
        "                    ha=\"center\", va=\"bottom\")\n",
        "\n",
        "add_values(bars1)\n",
        "add_values(bars2)\n",
        "add_values(bars3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ThGlCaL43E5g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uBuQSEDpdFzm"
      },
      "source": [
        "# **Ensemble Predcition with visualization of the best predcited model-image**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bbMYVWSMa5It"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "import datasets.transforms as T\n",
        "import torch\n",
        "from torchvision.ops import nms\n",
        "\n",
        "# Define the image transformation\n",
        "transform = T.Compose([\n",
        "    T.RandomResize([800], max_size=1333),\n",
        "    T.ToTensor(),\n",
        "    T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Load and transform the image\n",
        "image_path = \"/content/DINO/6458.jpg\"\n",
        "image = Image.open(image_path).convert(\"RGB\")\n",
        "image, _ = transform(image, None)\n",
        "\n",
        "# Define checkpoints\n",
        "checkpoints = {\n",
        "\n",
        "    \"checkpoint.pth\": \"/content/drive/MyDrive/checkpoint.pth\",\n",
        "    \"checkpoint24.pth\":\"/content/drive/MyDrive/checkpoint24norm.pth\",\n",
        "    \"checkpoint36.pth\":\"/content/drive/MyDrive/checkpoint36norm.pth\",\n",
        "    \"checkpoint12withaug.pth\": \"/content/drive/MyDrive/checkpoint12.pth\",\n",
        "    \"checkpoint24withaug.pth\": \"/content/drive/MyDrive/checkpoint24.pth\",\n",
        "    \"checkpoint36withaug.pth\": \"/content/drive/MyDrive/checkpoint36.pth\"\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "# Define id2name for the single class\n",
        "id2name = {1: \"person\"}  # Class ID for \"person\" is 1\n",
        "\n",
        "# Initialize best checkpoint variables\n",
        "best_checkpoint = None\n",
        "best_person_count = 0\n",
        "best_pred_dict = None\n",
        "\n",
        "# Function to filter bounding boxes for each unique person index\n",
        "def filter_duplicate_boxes(boxes, scores, iou_threshold=0.5):\n",
        "    selected_indices = nms(boxes, scores, iou_threshold)\n",
        "    return boxes[selected_indices], scores[selected_indices]\n",
        "\n",
        "for title, checkpoint_path in checkpoints.items():\n",
        "    # Load the model for each checkpoint\n",
        "    model, criterion, postprocessors = build_model_main(args)\n",
        "    checkpoint = torch.load(checkpoint_path, map_location='cpu')\n",
        "    model.load_state_dict(checkpoint['model'])\n",
        "    model.eval()  # Set to evaluation mode\n",
        "\n",
        "    # Run the model\n",
        "    output = model.cuda()(image[None].cuda())\n",
        "    output = postprocessors['bbox'](output, torch.Tensor([[1.0, 1.0]]).cuda())[0]\n",
        "\n",
        "    # Apply confidence threshold and label filter\n",
        "    threshold = 0.3\n",
        "    scores = output['scores']\n",
        "    labels = output['labels']\n",
        "    boxes = output['boxes']  # Use boxes directly for NMS (in xyxy format)\n",
        "\n",
        "    # Select only \"person\" detections\n",
        "    select_mask = (scores > threshold) & (labels == 1)\n",
        "    selected_boxes = boxes[select_mask]\n",
        "    selected_scores = scores[select_mask]\n",
        "\n",
        "    # Filter for unique bounding boxes per person\n",
        "    unique_boxes, unique_scores = filter_duplicate_boxes(selected_boxes, selected_scores)\n",
        "\n",
        "    # Count the unique persons detected\n",
        "    detected_person_count = len(unique_boxes)\n",
        "\n",
        "    print(f\"Checkpoint: {title}, Detected People Count (unique): {detected_person_count}\")\n",
        "\n",
        "    if detected_person_count > best_person_count:\n",
        "        best_person_count = detected_person_count\n",
        "        best_checkpoint = title\n",
        "        box_label = [id2name[1] for _ in range(detected_person_count)]\n",
        "        best_pred_dict = {\n",
        "            'boxes': unique_boxes.cpu(),\n",
        "            'size': torch.Tensor([image.shape[1], image.shape[2]]),\n",
        "            'box_label': box_label\n",
        "        }\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k5utGJera_Eo"
      },
      "outputs": [],
      "source": [
        "# Replace model_checkpoint_path with the best checkpoint found in Cell 1\n",
        "model_config_path = \"config/DINO/DINO_4scale.py\"\n",
        "model_checkpoint_path = checkpoints[best_checkpoint]  # Path of the best checkpoint\n",
        "\n",
        "# Load the model with best checkpoint\n",
        "args = SLConfig.fromfile(model_config_path)\n",
        "args.device = 'cuda'\n",
        "model, criterion, postprocessors = build_model_main(args)\n",
        "checkpoint = torch.load(model_checkpoint_path, map_location='cpu')\n",
        "model.load_state_dict(checkpoint['model'])\n",
        "_ = model.eval()\n",
        "\n",
        "# Run the model on the image\n",
        "output = model.cuda()(image[None].cuda())\n",
        "output = postprocessors['bbox'](output, torch.Tensor([[1.0, 1.0]]).cuda())[0]\n",
        "\n",
        "# Set threshold and initialize visualizer\n",
        "threshold = 0.3\n",
        "vslzr = COCOVisualizer()\n",
        "\n",
        "# Filter detections based on the threshold\n",
        "scores = output['scores']\n",
        "labels = output['labels']\n",
        "boxes = box_ops.box_xyxy_to_cxcywh(output['boxes'])\n",
        "select_mask = scores > threshold\n",
        "\n",
        "# Prepare prediction dictionary for visualization\n",
        "box_label = [id2name[int(item)] for item in labels[select_mask]]\n",
        "pred_dict = {\n",
        "    'boxes': boxes[select_mask],\n",
        "    'size': torch.Tensor([image.shape[1], image.shape[2]]),\n",
        "    'box_label': box_label\n",
        "}\n",
        "\n",
        "# Visualize the predictions\n",
        "vslzr.visualize(image, pred_dict, savedir=None, dpi=100)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UY8j4iY_3dvj"
      },
      "source": [
        "# **Ground truth image generator**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "koDj3CaTXNm_"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "from PIL import Image, ImageDraw\n",
        "from IPython.display import display\n",
        "\n",
        "# Path to the COCO annotation file\n",
        "annotation_file = '/content/DINO/random_sample_mavi_2_gt.json'  # Adjust this path if necessary\n",
        "image_dir = '/content/DINO/Pedestrian_dataset_for_internship_assignment'  # Path to the image dataset\n",
        "\n",
        "# Load the annotations file\n",
        "with open(annotation_file, 'r') as f:\n",
        "    coco_data = json.load(f)\n",
        "\n",
        "# Define the image filename you want to visualize\n",
        "target_image_filename = '6962.jpg'  # Update this with the actual filename\n",
        "\n",
        "# Find the image ID and details for the target image\n",
        "target_image = next((img for img in coco_data['images'] if img['file_name'] == target_image_filename), None)\n",
        "\n",
        "if target_image:\n",
        "    image_id = target_image['id']\n",
        "    image_path = os.path.join(image_dir, target_image_filename)\n",
        "\n",
        "    # Load the image\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "    draw = ImageDraw.Draw(image)\n",
        "\n",
        "    # Find annotations (bounding boxes) for the target image\n",
        "    annotations = [ann for ann in coco_data['annotations'] if ann['image_id'] == image_id]\n",
        "\n",
        "    # Draw each bounding box on the image\n",
        "    for ann in annotations:\n",
        "        bbox = ann['bbox']\n",
        "        # COCO format is [x, y, width, height]; convert to [x0, y0, x1, y1]\n",
        "        x0, y0, width, height = bbox\n",
        "        x1, y1 = x0 + width, y0 + height\n",
        "        draw.rectangle([x0, y0, x1, y1], outline=\"red\", width=3)\n",
        "\n",
        "    # Display the image with bounding boxes in Colab\n",
        "    display(image)\n",
        "else:\n",
        "    print(f\"Image with filename '{target_image_filename}' not found in the annotations.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MHNhIRUS3j9X"
      },
      "source": [
        "# **Visualizing Attention maps**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gGP49wQQgcCB"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "import datasets.transforms as T\n",
        "\n",
        "# Assuming the visualizer code provided in `visualizer.py` is already defined\n",
        "vslzr = COCOVisualizer()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9RZw2GRill1Z",
        "outputId": "e435b02d-ca1f-484b-9855-29ac417980e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/DINO\n"
          ]
        }
      ],
      "source": [
        "%cd /content/DINO"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*the below cell is to analyze the models architecture*"
      ],
      "metadata": {
        "id": "UzdheFZbAhN-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p9RWadJEggqG"
      },
      "outputs": [],
      "source": [
        "# Define model configuration and checkpoint paths\n",
        "model_config_path = \"config/DINO/DINO_4scale.py\"\n",
        "model_checkpoint_path = \"/content/drive/MyDrive/checkpoint36.pth\"\n",
        "# Load model configuration\n",
        "args = SLConfig.fromfile(model_config_path)\n",
        "args.device = 'cuda'\n",
        "\n",
        "# Build and load the model with the checkpoint\n",
        "model, criterion, postprocessors = build_model_main(args)\n",
        "checkpoint = torch.load(model_checkpoint_path, map_location='cpu')\n",
        "model.load_state_dict(checkpoint['model'])\n",
        "model.eval()  # Set model to evaluation mode\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WkIFTn_bIIHE"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import torch\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.functional as F\n",
        "from main import build_model_main\n",
        "from util.slconfig import SLConfig\n",
        "from datasets import build_dataset\n",
        "from util.visualizer import COCOVisualizer\n",
        "from util import box_ops\n",
        "import datasets.transforms as T\n",
        "\n",
        "# Config and paths\n",
        "model_config_path = \"config/DINO/DINO_4scale.py\"\n",
        "fine_tuned_checkpoint_path = \"/content/drive/MyDrive/checkpoint12.pth\"\n",
        "image_dir = \"/content/DINO/COCODIR/val2017\"\n",
        "\n",
        "# Load model configuration and build model\n",
        "args = SLConfig.fromfile(model_config_path)\n",
        "args.device = 'cuda'\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Build and load the model with checkpoint\n",
        "model, criterion, postprocessors = build_model_main(args)\n",
        "checkpoint = torch.load(fine_tuned_checkpoint_path, map_location='cpu')\n",
        "model.load_state_dict(checkpoint['model'])\n",
        "model.eval().to(device)\n",
        "\n",
        "# Prepare input image transformation\n",
        "transform = T.Compose([\n",
        "    T.RandomResize([800], max_size=1333),\n",
        "    T.ToTensor(),\n",
        "    T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Dictionary to hold attention weights for each layer\n",
        "attention_weights = {}\n",
        "decoder_layers = [f\"transformer.decoder.layers.{i}\" for i in range(6)]\n",
        "\n",
        "# Register hooks to capture attention weights in decoder layers\n",
        "def register_hooks(model):\n",
        "    for name, module in model.named_modules():\n",
        "        if hasattr(module, 'self_attn') and name in decoder_layers:\n",
        "            module.self_attn.register_forward_hook(\n",
        "                lambda mod, inp, out, name=name: attention_weights.setdefault(name, []).append(out[0].detach().cpu().numpy())\n",
        "            )\n",
        "\n",
        "register_hooks(model)\n",
        "\n",
        "# Load 5 random images from the specified directory\n",
        "image_files = random.sample(os.listdir(image_dir), 5)\n",
        "\n",
        "for image_file in image_files:\n",
        "    image_path = os.path.join(image_dir, image_file)\n",
        "\n",
        "    # Prepare input image\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "    transformed_image, _ = transform(image, None)\n",
        "\n",
        "    # Run inference and capture attention weights\n",
        "    with torch.no_grad():\n",
        "        output = model(transformed_image[None].to(device))\n",
        "        bbox_output = postprocessors['bbox'](output, torch.Tensor([[1.0, 1.0]]).to(device))[0]\n",
        "\n",
        "    # Display original image\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    plt.imshow(image)\n",
        "    plt.axis('off')\n",
        "    plt.title(f\"Original Image - {image_file}\")\n",
        "    plt.show()\n",
        "\n",
        "    # Visualize attention maps for each decoder layer from 0 to 5\n",
        "    for layer in decoder_layers:\n",
        "        if layer in attention_weights:\n",
        "            attn_map = attention_weights[layer][0][0, 0]  # Only head 0\n",
        "            spatial_size = int(np.sqrt(attn_map.shape[-1]))\n",
        "            if spatial_size * spatial_size == attn_map.shape[-1]:  # Ensure it's a square attention map\n",
        "                attn_map = attn_map.reshape(spatial_size, spatial_size)\n",
        "\n",
        "                # Interpolate to match the exact original image size\n",
        "                attn_map = torch.tensor(attn_map).unsqueeze(0).unsqueeze(0)\n",
        "                attn_map = F.interpolate(attn_map, size=(image.size[1], image.size[0]), mode='bilinear', align_corners=False)\n",
        "                attn_map = attn_map.squeeze().numpy()\n",
        "\n",
        "                # Display the attention map overlay with correct scaling\n",
        "                plt.figure(figsize=(10, 10))\n",
        "                plt.imshow(np.array(image) / 255.0, interpolation='none')\n",
        "                plt.imshow(attn_map, cmap='viridis', alpha=0.6)\n",
        "                plt.axis('off')\n",
        "                plt.title(f\"Attention Map - {layer} - {image_file}\")\n",
        "                plt.show()\n",
        "\n",
        "    # Clear attention weights for the next image\n",
        "    attention_weights.clear()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **OUTPUT-with ensemble bounding box predcition with attention map**"
      ],
      "metadata": {
        "id": "2qglugp7AoZr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Run the below cells to visulize the bounding box and the attention map**"
      ],
      "metadata": {
        "id": "xpwmCHgeA-8s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import datasets.transforms as T\n",
        "import torch\n",
        "from torchvision.ops import nms\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from main import build_model_main\n",
        "from util.slconfig import SLConfig\n",
        "from util.visualizer import COCOVisualizer\n",
        "from util import box_ops\n",
        "from datasets import build_dataset\n",
        "\n",
        "\n",
        "model_config_path = \"config/DINO/DINO_4scale.py\"\n",
        "\n",
        "\n",
        "image_path = \"/content/DINO/3276.jpg\" # replace your image path here\n",
        "\n",
        "\n",
        "checkpoints = {\n",
        "    \"checkpoint.pth\": \"/content/drive/MyDrive/checkpoint.pth\",\n",
        "    \"checkpoint36.pth\": \"/content/drive/MyDrive/checkpoint36norm.pth\",\n",
        "    \"checkpoint12withaug.pth\": \"/content/drive/MyDrive/checkpoint12.pth\",\n",
        "    \"checkpoint36withaug.pth\": \"/content/drive/MyDrive/checkpoint36.pth\"\n",
        "}\n",
        "\n",
        "\n",
        "id2name = {1: \"person\"}\n"
      ],
      "metadata": {
        "id": "jI10iurdCMFM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "transform = T.Compose([\n",
        "    T.RandomResize([800], max_size=1333),\n",
        "    T.ToTensor(),\n",
        "    T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "\n",
        "image = Image.open(image_path).convert(\"RGB\")\n",
        "image, _ = transform(image, None)\n",
        "\n",
        "\n",
        "best_checkpoint = None\n",
        "best_person_count = 0\n",
        "best_pred_dict = None\n",
        "\n",
        "\n",
        "def filter_duplicate_boxes(boxes, scores, iou_threshold=0.5):\n",
        "    selected_indices = nms(boxes, scores, iou_threshold)\n",
        "    return boxes[selected_indices], scores[selected_indices]\n",
        "\n",
        "\n",
        "for title, checkpoint_path in checkpoints.items():\n",
        "    # Load the model for each checkpoint\n",
        "    args = SLConfig.fromfile(model_config_path)\n",
        "    args.device = 'cuda'\n",
        "    model, criterion, postprocessors = build_model_main(args)\n",
        "    checkpoint = torch.load(checkpoint_path, map_location='cpu')\n",
        "    model.load_state_dict(checkpoint['model'])\n",
        "    model.eval().cuda()\n",
        "\n",
        "\n",
        "    output = model(image[None].cuda())\n",
        "    output = postprocessors['bbox'](output, torch.Tensor([[1.0, 1.0]]).cuda())[0]\n",
        "\n",
        "    threshold = 0.3\n",
        "    scores = output['scores']\n",
        "    labels = output['labels']\n",
        "    boxes = output['boxes']\n",
        "\n",
        "    select_mask = (scores > threshold) & (labels == 1)\n",
        "    selected_boxes = boxes[select_mask]\n",
        "    selected_scores = scores[select_mask]\n",
        "\n",
        "    unique_boxes, unique_scores = filter_duplicate_boxes(selected_boxes, selected_scores)\n",
        "    detected_person_count = len(unique_boxes)\n",
        "\n",
        "    print(f\"Checkpoint: {title}, Detected People Count (unique): {detected_person_count}\")\n",
        "\n",
        "\n",
        "    if detected_person_count > best_person_count:\n",
        "        best_person_count = detected_person_count\n",
        "        best_checkpoint = title\n",
        "        box_label = [id2name[1] for _ in range(detected_person_count)]\n",
        "        best_pred_dict = {\n",
        "            'boxes': unique_boxes.cpu(),\n",
        "            'size': torch.Tensor([image.shape[1], image.shape[2]]),\n",
        "            'box_label': box_label\n",
        "        }\n",
        "\n",
        "print(f\"Best performing checkpoint: {best_checkpoint} with detected people count: {best_person_count}\")\n",
        "model_checkpoint_path = checkpoints[best_checkpoint]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yy-0h4KACofL",
        "outputId": "22a137e2-7833-4c5e-c931-6cc914ab44c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-102-6dcc908a5ac6>:27: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(checkpoint_path, map_location='cpu')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checkpoint: checkpoint.pth, Detected People Count (unique): 5\n",
            "Checkpoint: checkpoint36.pth, Detected People Count (unique): 9\n",
            "Checkpoint: checkpoint12withaug.pth, Detected People Count (unique): 8\n",
            "Checkpoint: checkpoint36withaug.pth, Detected People Count (unique): 6\n",
            "Best performing checkpoint: checkpoint36.pth with detected people count: 9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Visualizing bounding box detection**"
      ],
      "metadata": {
        "id": "2nTQADAyDdUW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "args = SLConfig.fromfile(model_config_path)\n",
        "args.device = 'cuda'\n",
        "model, criterion, postprocessors = build_model_main(args)\n",
        "checkpoint = torch.load(model_checkpoint_path, map_location='cpu')\n",
        "model.load_state_dict(checkpoint['model'])\n",
        "model.eval().cuda()\n",
        "\n",
        "\n",
        "output = model(image[None].cuda())\n",
        "output = postprocessors['bbox'](output, torch.Tensor([[1.0, 1.0]]).cuda())[0]\n",
        "threshold = 0.3\n",
        "scores = output['scores']\n",
        "labels = output['labels']\n",
        "boxes = box_ops.box_xyxy_to_cxcywh(output['boxes'])\n",
        "select_mask = scores > threshold\n",
        "box_label = [id2name[int(item)] for item in labels[select_mask]]\n",
        "pred_dict = {\n",
        "    'boxes': boxes[select_mask],\n",
        "    'size': torch.Tensor([image.shape[1], image.shape[2]]),\n",
        "    'box_label': box_label\n",
        "}\n",
        "\n",
        "\n",
        "vslzr = COCOVisualizer()\n",
        "vslzr.visualize(image, pred_dict, savedir=None, dpi=100)\n"
      ],
      "metadata": {
        "id": "zOZG8a1BCysY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Visualizing attention maps**"
      ],
      "metadata": {
        "id": "PjT0OJqZEMIp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "args = SLConfig.fromfile(model_config_path)\n",
        "args.device = 'cuda'\n",
        "model, criterion, postprocessors = build_model_main(args)\n",
        "checkpoint = torch.load(model_checkpoint_path, map_location='cpu')\n",
        "model.load_state_dict(checkpoint['model'])\n",
        "model.eval().cuda()\n",
        "\n",
        "attention_weights = {}\n",
        "decoder_layers = [f\"transformer.decoder.layers.{i}\" for i in range(6)]\n",
        "\n",
        "\n",
        "def register_hooks(model):\n",
        "    for name, module in model.named_modules():\n",
        "        if hasattr(module, 'self_attn') and name in decoder_layers:\n",
        "            module.self_attn.register_forward_hook(\n",
        "                lambda mod, inp, out, name=name: attention_weights.setdefault(name, []).append(out[0].detach().cpu().numpy())\n",
        "            )\n",
        "\n",
        "register_hooks(model)\n",
        "\n",
        "\n",
        "image = Image.open(image_path).convert(\"RGB\")\n",
        "transformed_image, _ = transform(image, None)\n",
        "\n",
        "\n",
        "with torch.no_grad():\n",
        "    output = model(transformed_image[None].cuda())\n",
        "    bbox_output = postprocessors['bbox'](output, torch.Tensor([[1.0, 1.0]]).cuda())[0]\n",
        "\n",
        "\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.imshow(image)\n",
        "plt.axis('off')\n",
        "plt.title(\"Original Image\")\n",
        "plt.show()\n",
        "\n",
        "\n",
        "fig, axes = plt.subplots(1, 6, figsize=(20, 5))\n",
        "for idx, layer in enumerate(decoder_layers):\n",
        "    if layer in attention_weights:\n",
        "        attn_map = attention_weights[layer][0][0, 0]\n",
        "        spatial_size = int(np.sqrt(attn_map.shape[-1]))\n",
        "        if spatial_size * spatial_size == attn_map.shape[-1]:\n",
        "            attn_map = attn_map.reshape(spatial_size, spatial_size)\n",
        "            attn_map = F.interpolate(torch.tensor(attn_map).unsqueeze(0).unsqueeze(0),\n",
        "                                     size=(image.size[1], image.size[0]),\n",
        "                                     mode='bilinear', align_corners=False).squeeze().numpy()\n",
        "\n",
        "\n",
        "            axes[idx].imshow(image, alpha=0.6)\n",
        "            axes[idx].imshow(attn_map, cmap='viridis', alpha=0.6)\n",
        "            axes[idx].axis('off')\n",
        "            axes[idx].set_title(f\"Layer {idx}\")\n",
        "\n",
        "plt.show()\n",
        "attention_weights.clear()\n"
      ],
      "metadata": {
        "id": "aBFuqesZC06i"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}